{"cells":[{"cell_type":"markdown","id":"a47acaff-7667-4671-a05e-a31c12f4f48c","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"6788a553-6ce3-40c6-8bc5-99d84fa688ce","metadata":{},"outputs":[],"source":["# Machine Learning Foundation\n","\n","## Course 5, Part h: Autoencoders LAB\n"]},{"cell_type":"markdown","id":"19cdb746-cc0c-458e-ae68-79728774bc73","metadata":{},"outputs":[],"source":["## Learning Objectives \n","\n","1. Implement the following dimensionality reduction techniques on the MNIST data:\n","    * PCA\n","    * Autoencoders\n","    * Variational autoencoders\n","2. Use appropriate scoring metrics to compare the performance of each.\n"]},{"cell_type":"markdown","id":"f0da00cf-e616-49f8-b2dd-cec138a8499b","metadata":{},"outputs":[],"source":["**Note:** Please run the following cell to upgrade tensorflow to avoid warnings and importing errors in this lab. **RESTART THE KERNEL after the upgrade.**\n"]},{"cell_type":"code","id":"60f55d8a-e906-4f9e-be7a-48d1e8babc27","metadata":{},"outputs":[],"source":["%%capture\n!pip install --upgrade tensorflow "]},{"cell_type":"markdown","id":"50b6f9d0-5a3b-4bf4-9ebd-9deaafca59a0","metadata":{},"outputs":[],"source":["Make sure that the version of tensorflow is greater than 2.9.0.\n"]},{"cell_type":"code","id":"fa734cd8-70b3-4c17-9905-790fa713f08c","metadata":{},"outputs":[],"source":["import warnings\nwarnings.simplefilter(\"ignore\")\n\nimport tensorflow as tf\nprint(tf.__version__)"]},{"cell_type":"markdown","id":"4cdcc380-5fea-4df7-b6a9-92a691813be2","metadata":{},"outputs":[],"source":["## Importing Required Libraries\n"]},{"cell_type":"code","id":"a22b6e78-9419-4eaa-aebd-bce02123846c","metadata":{},"outputs":[],"source":["from tensorflow import keras\nfrom keras.datasets import mnist\nimport numpy as np\nnp.set_printoptions(precision=2)\n\nimport matplotlib.pyplot as plt"]},{"cell_type":"markdown","id":"7ea4340c-9962-44a7-bd86-fe3995c3bcff","metadata":{},"outputs":[],"source":["## MNIST data\n"]},{"cell_type":"markdown","id":"ffea0046-5ca3-4813-931d-2892081a60af","metadata":{},"outputs":[],"source":["Throughout this lesson, we'll be working with the famous [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), which contains 70,000 handwritten black-and-white images, which are traditionally split into 60k training images and 10k validation images.\n"]},{"cell_type":"code","id":"dfc15574-f80f-4d2f-bc67-4c11ae0afc27","metadata":{},"outputs":[],"source":["(x_train, y_train), (x_test, y_test) = mnist.load_data();"]},{"cell_type":"markdown","id":"486a1fda-be38-49d2-856f-e46a4a7474bb","metadata":{},"outputs":[],"source":["### Preprocess MNIST\n"]},{"cell_type":"markdown","id":"05d620fb-ae24-48ea-9900-ec50dc1ea1f1","metadata":{},"outputs":[],"source":["We'll preprocess by scaling images pixels to be between 0 and 1.\n"]},{"cell_type":"code","id":"cb588c00-f736-4a54-9d5f-469ca9c6c259","metadata":{},"outputs":[],"source":["x_train = x_train.astype('float') / 255.\nx_test = x_test.astype('float') / 255."]},{"cell_type":"markdown","id":"d82d0363-e0bf-454a-93d3-1f620c4b952c","metadata":{},"outputs":[],"source":["### Part 1: PCA\n"]},{"cell_type":"markdown","id":"79935dbb-a51c-4827-98f3-a4d4ed90d432","metadata":{},"outputs":[],"source":["We'll use PCA as a baseline with which we can compare our deep learning models.\n"]},{"cell_type":"markdown","id":"0ffda9e9-558f-464b-999d-2822e115cf4d","metadata":{},"outputs":[],"source":["For PCA we'll treat each image like a \"row\" of data and flatten our data, so each observation is 784 columns:\n"]},{"cell_type":"code","id":"1e97d9c6-3d80-4207-9155-8dee4a830021","metadata":{},"outputs":[],"source":["x_train_flat = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test_flat = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\nprint(x_train_flat.shape)\nprint(x_test_flat.shape)"]},{"cell_type":"markdown","id":"e3a556ee-6047-43dd-9c92-e4bfb104f557","metadata":{},"outputs":[],"source":["Recall that PCA will do a **matrix decomposition** of the data to find the **eigenvectors** - these **eigenvectors** will be the **principal components** of the data, or the \"latent features\" that describe a maximal amount of variance in the data. We will sort the eigenvectors by their eigenvalues, and those eigenvectors with the largest eigenvalues (the scales) will be selected as the principal components to achieve dimensionality reduction.\n"]},{"cell_type":"markdown","id":"49c3c8be-12f6-41b8-966d-2be967ba26ab","metadata":{},"outputs":[],"source":["PCA works better if our features are scaled, so let's do that. \n"]},{"cell_type":"code","id":"a05435da-1a8f-4c9d-8dbf-9573ed84d740","metadata":{},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\ns = MinMaxScaler().fit(x_train_flat)\nx_train_scaled = s.transform(x_train_flat)"]},{"cell_type":"code","id":"b5f23e27-ec0e-4776-9da6-441a773e132c","metadata":{},"outputs":[],"source":["from sklearn.decomposition import PCA\n\ndef mnist_pca(x_data, n_components):\n    pca = PCA(n_components=n_components)\n\n    fit_pca = pca.fit(x_data)\n    \n    print(\"Variance explained with {0} components:\".format(n_components), \n          round(sum(fit_pca.explained_variance_ratio_), 2))\n\n    return fit_pca, fit_pca.transform(x_data)"]},{"cell_type":"code","id":"1d8c920c-0e5f-467e-9cc6-31decd42ca0b","metadata":{},"outputs":[],"source":["pca_full, mnist_data_full = mnist_pca(x_train_scaled, 784)"]},{"cell_type":"code","id":"c171243c-fd90-4f91-9488-b65e0200ef08","metadata":{},"outputs":[],"source":["plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\nplt.title(\"Proportion of PCA variance\\nexplained by number of components\")\nplt.xlabel(\"Number of components\")\nplt.ylabel(\"Proportion of variance explained\");"]},{"cell_type":"markdown","id":"ee331879-415a-46e1-b515-78104bf68ebd","metadata":{},"outputs":[],"source":["We need about 250 components to explain 90% of the variance in the data!\n"]},{"cell_type":"markdown","id":"5d19b5e9-ef15-4b72-836e-6348d46ea709","metadata":{},"outputs":[],"source":["For visualization purposes, let's extract just two components and plot them:\n"]},{"cell_type":"code","id":"d1881f7f-1e1e-43d8-9efe-eac88abff73c","metadata":{},"outputs":[],"source":["pca_2, mnist_data_2 = mnist_pca(x_train_scaled, 2)"]},{"cell_type":"code","id":"14aca812-5c9e-4610-9875-921f238cac70","metadata":{},"outputs":[],"source":["num_images_per_class = 250\nfig = plt.figure(figsize=(12,12))\nfor number in list(range(10)):\n    mask = y_train == number\n    x_data = mnist_data_2[mask, 0][:num_images_per_class]\n    y_data = mnist_data_2[mask, 1][:num_images_per_class]\n    plt.scatter(x_data, y_data, label=number, alpha=1)\nplt.legend();"]},{"cell_type":"markdown","id":"3e995d62-a383-4068-b793-2ad7ca6ea61c","metadata":{},"outputs":[],"source":["We can already see that the latent features PCA is learning _somewhat_ disentangle the features here, and a neural network could certainly help with this.\n"]},{"cell_type":"markdown","id":"436047e3-6ec8-4484-b986-7d3c67ec5361","metadata":{},"outputs":[],"source":["### Scoring PCA\n"]},{"cell_type":"markdown","id":"0dc2e814-7ca5-4c4e-8a8c-552dfe43b836","metadata":{},"outputs":[],"source":["What we really care about is: how good is PCA at creating a low dimensional representation of the MNIST data?\n","\n","As with all models, we should test performance on a different dataset than we trained on:\n"]},{"cell_type":"code","id":"76a1440e-6bfe-4bf5-8cde-1f871ed0aa4c","metadata":{},"outputs":[],"source":["pca_64, mnist_data_64 = mnist_pca(x_train_scaled, 64)"]},{"cell_type":"code","id":"1f6f4d7c-0e98-4f51-b491-e39b90877b4d","metadata":{},"outputs":[],"source":["\nx_test_scaled = s.transform(x_test_flat)"]},{"cell_type":"code","id":"f9a7f027-5798-4c04-aa9b-b4180ba18704","metadata":{},"outputs":[],"source":["x_test_flat_64 = pca_64.transform(x_test_scaled)\nx_test_reconstructed_64 = pca_64.inverse_transform(x_test_flat_64)"]},{"cell_type":"code","id":"04bec341-0776-4e3f-974d-7c7a5a3ac824","metadata":{},"outputs":[],"source":["x_test_reconstructed_64.shape"]},{"cell_type":"code","id":"7c7fbf6a-ae49-441e-baf8-6ab7e114ac00","metadata":{},"outputs":[],"source":["true = x_test_scaled\nreconstructed = x_test_reconstructed_64"]},{"cell_type":"code","id":"07cd06c6-3e73-4786-88e2-3a5c29aba34e","metadata":{},"outputs":[],"source":["def mse_reconstruction(true, reconstructed):\n    return np.sum(np.power(true - reconstructed, 2) / true.shape[1])"]},{"cell_type":"code","id":"8918ce8f-f660-4425-aebc-7bf0cc7d3ef6","metadata":{},"outputs":[],"source":["mse_reconstruction(true, reconstructed)"]},{"cell_type":"markdown","id":"c3190ca5-d4bf-4446-9643-04c6420e3383","metadata":{},"outputs":[],"source":["Average MSE of **90.6**, using 64 components, for PCA.\n"]},{"cell_type":"markdown","id":"a0e01150-32cf-4f18-b432-2bd9e09e70dd","metadata":{},"outputs":[],"source":["### Simple AE\n"]},{"cell_type":"markdown","id":"0744b603-a818-47e8-bc09-2d6a101015db","metadata":{},"outputs":[],"source":["Now let's build an autoencoder! Fundamentally, an autoencoder is an neural network where the input is the same as the output. The hope for such networks is that one of the hidden layers will \"learn a compressed representation\" of the data, similar to the way PCA does.\n"]},{"cell_type":"code","id":"301f3be3-e6c5-4b37-8e78-c3ebe4f1e97b","metadata":{},"outputs":[],"source":["from keras.layers import Input, Dense\nfrom keras.models import Model"]},{"cell_type":"markdown","id":"bb90ee16-b975-4c3a-bec5-623860b60f00","metadata":{},"outputs":[],"source":["In practice, when implementing autoencoders using `Keras`, we define _three_ models:\n","\n","* The \"full autoencoder\", here `full_model`, will take inputs and try to reconstruct them\n","* The \"encoder\", here `encoder_model`, will take inputs and try to reconstruct them\n","* The \"decoder\", here `decoder_model`, will take the latent space and try to reconstruct it\n"]},{"cell_type":"code","id":"60e1ee89-f8de-412e-b9f6-08a3e741ced4","metadata":{},"outputs":[],"source":["ENCODING_DIM = 64\n\n# Encoder model\ninputs = Input(shape=(784,)) \nencoded = Dense(ENCODING_DIM, activation=\"sigmoid\")(inputs)\nencoder_model = Model(inputs, encoded, name='encoder')\n\n# Decoder model\nencoded_inputs = Input(shape=(ENCODING_DIM,), name='encoding')\nreconstruction = Dense(784, activation=\"sigmoid\")(encoded_inputs)\ndecoder_model = Model(encoded_inputs, reconstruction, name='decoder')\n\n# Defining the full model as the combination of the two\noutputs = decoder_model(encoder_model(inputs))\nfull_model = Model(inputs, outputs, name='full_ae')"]},{"cell_type":"code","id":"dd93c44e-8dfc-42d8-935d-6139cfdbbf4a","metadata":{},"outputs":[],"source":["full_model.compile(optimizer='rmsprop',\n                 loss='binary_crossentropy',\n                 metrics=['accuracy'])"]},{"cell_type":"code","id":"a53dffa2-8d98-474f-ba90-d9e521c9fdd3","metadata":{},"outputs":[],"source":["history = full_model.fit(x_train_flat, x_train_flat, shuffle=True, epochs=1, batch_size=32)"]},{"cell_type":"markdown","id":"18bbbeb6-a26a-4c42-a5bb-134de82795e8","metadata":{},"outputs":[],"source":["Feel free to do `.summary` and `visual` on this trained model now.\n"]},{"cell_type":"markdown","id":"f42530a1-c733-4fab-a662-5c58ddea5f31","metadata":{},"outputs":[],"source":["Recall that with Keras, training the model with layers 1-5 trains the smaller model simultaneously, as long as they are built off of the same underlying objects!\n"]},{"cell_type":"code","id":"d54d82da-e504-40c7-b4fd-bfa425a9df77","metadata":{},"outputs":[],"source":["encoded_images = encoder_model.predict(x_test_flat)\nencoded_images.shape"]},{"cell_type":"markdown","id":"8d698fd9-1c81-4782-8cb3-87f2f9c0b072","metadata":{},"outputs":[],"source":["So, the \"encoder model\" has already been trained! For example, the encoding for the first image in the test dataset is:\n"]},{"cell_type":"code","id":"938f16c4-2f7e-4d53-b01f-083f918c7f7c","metadata":{},"outputs":[],"source":["encoded_images[0]"]},{"cell_type":"markdown","id":"cd6e2594-f7fb-4e9a-85b7-870ba814ff32","metadata":{},"outputs":[],"source":["As we can see, the `encoder_model` has been trained!\n"]},{"cell_type":"markdown","id":"1d4ea00b-489c-4260-8daa-4c2804bd91a7","metadata":{},"outputs":[],"source":["### Exercise 1\n","\n","With PCA, we compared the pixel-wise difference between the reconstructed images and the original images to score how good the model was. \n","\n","Your task is now to do the same using the autoencoder. You'll have to think about what needs to get fed through the autoencoder to do this. Your steps are:\n","\n","1. Use the trained autoencoder to generate reconstructed images.\n","2. Compute the pixel-wise distance between the reconstructed images and the original images. \n","\n","Is your result higher or lower than what you got with PCA?\n"]},{"cell_type":"code","id":"3086aa71-c23c-4db3-9e86-195b1b74c02b","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\n# Generate reconstructed images\ndecoded_images = full_model.predict(x_test_flat)\nmse_reconstruction(decoded_images, x_test_flat)\n### END SOLUTION"]},{"cell_type":"markdown","id":"cf274beb-1dec-4003-b021-7bf0374f7be7","metadata":{},"outputs":[],"source":["Significantly worse!\n"]},{"cell_type":"markdown","id":"731933cb-3d93-4bbe-992d-d244c0f4bc91","metadata":{},"outputs":[],"source":["### Exercise 2:\n","\n","One of the reasons this model does worse than PCA is that is isn't a \"Deep Learning\" model. Go ahead and add an extra hidden Dense layer to both the `encoder_model` and the `decoder_model`.\n"]},{"cell_type":"code","id":"84e5f18e-4904-491c-98ba-d87e84bcfeb8","metadata":{},"outputs":[],"source":["ENCODING_DIM = 64\nHIDDEN_DIM = 256\n### BEGIN SOLUTION\n# Encoder model\ninputs = Input(shape=(784,))\nencoder_hidden = Dense(HIDDEN_DIM, activation=\"relu\")(inputs)  # Intermediate hidden layer\nencoded = Dense(ENCODING_DIM, activation=\"relu\")(encoder_hidden)  # Encoded representation\nencoder_model = Model(inputs, encoded, name='encoder')\n\n# Decoder model\nencoded_inputs = Input(shape=(ENCODING_DIM,), name='encoding')\ndecoder_hidden = Dense(HIDDEN_DIM, activation=\"relu\")(encoded_inputs)\nreconstruction = Dense(784, activation=\"sigmoid\")(decoder_hidden)\ndecoder_model = Model(encoded_inputs, reconstruction, name='decoder')\n\n# Defining the full model as the combination of the two\noutputs = decoder_model(encoder_model(inputs))\nfull_model = Model(inputs, outputs, name='full_ae')"]},{"cell_type":"code","id":"e125b611-c443-4f28-94f7-714a1a7216bc","metadata":{},"outputs":[],"source":["full_model.compile(optimizer='rmsprop',\n                 loss='binary_crossentropy',\n                 metrics=['accuracy'])"]},{"cell_type":"code","id":"4b4924f1-0287-4a20-b59f-6b931d161345","metadata":{},"outputs":[],"source":["history = full_model.fit(x_train_flat, x_train_flat, shuffle=True, epochs=5, batch_size=32)"]},{"cell_type":"code","id":"057d594a-b93b-4ef0-8e09-b9d2c5c02ee5","metadata":{},"outputs":[],"source":["# Generate reconstructed images\ndecoded_images = full_model.predict(x_test_flat)\nmse_reconstruction(decoded_images, x_test_flat)\n### END SOLUTION"]},{"cell_type":"markdown","id":"9f38cafb-bfa2-4b1c-ba26-9ae92ab1e5f8","metadata":{},"outputs":[],"source":["Better than PCA!\n"]},{"cell_type":"markdown","id":"50a04ab0-e948-400a-b2be-51e54ee8315a","metadata":{},"outputs":[],"source":["### Exercise 3:\n"]},{"cell_type":"markdown","id":"02a4cd0b-344e-4e21-9548-84d1b89d7883","metadata":{},"outputs":[],"source":["You've probably noticed we've only been training our model for one epoch. Let's test to see how our model's reconstruction loss decreases as we train it more. \n","\n","Write code that:\n","\n","1. Trains the autoencoder for 5 epochs\n","2. Prints the reconstruction loss after each epoch\n","\n","Does the reconstruction loss decrease after each epoch?\n"]},{"cell_type":"code","id":"982ff4bb-65e2-4ee7-bae0-874c7d28a1d2","metadata":{},"outputs":[],"source":["def train_ae_epochs(num_epochs=1):\n### BEGIN SOLUTION\n    ENCODING_DIM = 64\n    HIDDEN_DIM = 256\n\n      # Encoder model\n    inputs = Input(shape=(784,)) \n    encoder_hidden = Dense(HIDDEN_DIM, activation=\"relu\")(inputs)\n    encoded = Dense(ENCODING_DIM, activation=\"sigmoid\")(encoder_hidden)\n    encoder_model = Model(inputs, encoded, name='encoder')\n\n\n    # Decoder model\n    encoded_inputs = Input(shape=(ENCODING_DIM,), name='encoding')\n    decoder_hidden = Dense(HIDDEN_DIM, activation=\"relu\")(encoded_inputs)\n    reconstruction = Dense(784, activation=\"sigmoid\")(decoder_hidden)\n    decoder_model = Model(encoded_inputs, reconstruction, name='decoder')\n\n    # Defining the full model as the combination of the two\n    outputs = decoder_model(encoder_model(inputs))\n    full_model = Model(inputs, outputs, name='full_ae')\n    \n    full_model = Model(inputs=inputs, \n                       outputs=outputs)\n\n    full_model.compile(optimizer='rmsprop',\n                     loss='binary_crossentropy',\n                     metrics=['accuracy'])\n    mse_res = []\n    for i in range(num_epochs):\n        history = full_model.fit(x_train_flat, x_train_flat, shuffle=True, epochs=1, batch_size=32)\n    \n        decoded_images = full_model.predict(x_test_flat)\n        reconstruction_loss = mse_reconstruction(decoded_images, x_test_flat)\n        mse_res.append(reconstruction_loss)\n        print(\"Reconstruction loss after epoch {0} is {1}\"\n              .format(i+1, reconstruction_loss))\n### END SOLUTION       \n    return mse_res\n"]},{"cell_type":"code","id":"cfba1f0b-2137-4ff3-b493-41366baae387","metadata":{},"outputs":[],"source":["train_ae_epochs(5)"]},{"cell_type":"markdown","id":"a1e661b9-fdd0-4805-a036-b6d182cd42fb","metadata":{},"outputs":[],"source":["This is promising: even though the loss and accuracy don't appear to be changing much, as we train our autoencoder for longer, it does appear to be getting better and better at reconstructing the (unseen) test images.\n"]},{"cell_type":"markdown","id":"f205d2b2-b66d-4c3c-b7e6-672b3869b557","metadata":{},"outputs":[],"source":["### Variational autoencoder\n"]},{"cell_type":"markdown","id":"a5859ae9-0b37-4048-a139-acffdebb33e0","metadata":{},"outputs":[],"source":["VAEs are a way have neural networks learn representations of data, as with regular autoencoders. This time, however, the neural network will learn parameters of a normal distribution that will have observations drawn from it that will be transformed back into images. This results in two dimensional latent representation of the data once the variational autoencoder is trained, where one dimension represents the mean of the normal distribution and the other dimension represents the standard deviation.\n"]},{"cell_type":"markdown","id":"b2bb33b5-3cf8-4eb4-94d6-29cf62e4df2b","metadata":{},"outputs":[],"source":["At a high level, the actual steps of a VAE are:\n","\n","1. The first neural network, the \"encoder\", predicts two vectors for each image, which will then be interpreted as means and standard deviations and transformed into a normal distribution.\n","3. The second neural network, the \"decoder\", takes the results of this operation and tries to reconstruct the original image.\n","4. The entire system is trained with backpropagation. At each iteration, _two_ losses are computed:\n","    * One loss simply penalizes the system for producing images that don't match the original images.\n","    * The other loss penalizes the encoder model for not correctly producing statistics from the image that match a standard normal distribution. \n","   \n","The details are contained in the presentation. \n"]},{"cell_type":"code","id":"a0a70370-6cb1-4ca2-bec6-d2b5b9104f3f","metadata":{},"outputs":[],"source":["from keras.layers import Lambda, Input, Dense\nfrom keras.models import Model\nfrom keras.datasets import mnist\nfrom keras.losses import mse, binary_crossentropy\nfrom keras.utils import plot_model\nfrom keras import backend as K\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os"]},{"cell_type":"markdown","id":"633767e5-452e-4f3c-b3fc-4e753d3f650d","metadata":{},"outputs":[],"source":["The function below involves transforming the VAE encoder outputs into a normally distributed output. \n","\n","Recall that the VAE encoder generates vectors representing $\\mu$ and $log(\\sigma)$. These are then transformed into the generator input via \n","\n","$$ G_{in} = \\mu + \\sigma * \\epsilon $$\n","\n","where $ \\epsilon $ ~ $ N(0, 1) $.\n"]},{"cell_type":"code","id":"29f59cdf-392b-45a4-a04f-000d05dce7d5","metadata":{},"outputs":[],"source":["def sampling(args):\n    \"\"\"\n    Transforms parameters defining the latent space into a normal distribution.\n    \"\"\"\n    # Need to unpack arguments like this because of the way the Keras \"Lambda\" function works.\n    mu, log_sigma = args\n    # by default, random_normal has mean=0 and std=1.0\n    epsilon = K.random_normal(shape=tf.shape(mu))\n    sigma = K.exp(log_sigma)\n    return mu + K.exp(0.5 * sigma) * epsilon"]},{"cell_type":"code","id":"0806b23f-b3fb-4ec6-96e0-eda19c909e0a","metadata":{},"outputs":[],"source":["hidden_dim = 256\nbatch_size = 128\nlatent_dim = 2 \n# this is the dimension of each of the vectors representing the two parameters\n# that will get transformed into a normal distribution\nepochs = 1\n\n\n# VAE model = encoder + decoder\n# build encoder model\ninputs = Input(shape=(784, ), name='encoder_input')\nx = Dense(hidden_dim, activation='relu')(inputs)\n\n\nz_mean = Dense(latent_dim, name='z_mean')(x)\nz_log_var = Dense(latent_dim, name='z_log_var')(x)\n# NOTE: output of encoder model is *2* n-dimensional vectors:\n\nz = Lambda(sampling, name='z')([z_mean, z_log_var])\n# z is now one n dimensional vector representing the inputs \nencoder_model = Model(inputs, [z_mean, z_log_var, z], name='encoder')"]},{"cell_type":"markdown","id":"e321fd0d-ae53-49bf-b23c-7345b54e5426","metadata":{},"outputs":[],"source":["We'll have the `encoder_model` output `z_mean`, `z_log_var`, and `z` so we can plot the images as a function of these later.\n"]},{"cell_type":"code","id":"812e1617-6eab-42f1-8f3b-63a8e9a29bba","metadata":{},"outputs":[],"source":["# build decoder model\nlatent_inputs = Input(shape=(latent_dim,),)\nx = Dense(hidden_dim, activation='relu')(latent_inputs)\noutputs = Dense(784, activation='sigmoid')(x)\n\ndecoder_model = Model(latent_inputs, outputs, name='decoder')\n\n\n# instantiate VAE model\noutputs = decoder_model(encoder_model(inputs)[2])\nvae_model = Model(inputs, outputs, name='vae_mlp')"]},{"cell_type":"markdown","id":"9eeb38e7-55e7-47b6-ab89-012500d252de","metadata":{},"outputs":[],"source":["#### Examine each layer\n"]},{"cell_type":"code","id":"16d4b664-f7cf-4fc3-b2b9-c8dd958dcc0e","metadata":{},"outputs":[],"source":["for i, layer in enumerate(vae_model.layers):\n    print(\"Layer\", i+1)\n    print(\"Name\", layer.name)\n    print(\"Input shape\", layer.input_shape)\n    print(\"Output shape\", layer.output_shape)\n    if not layer.weights:\n        print(\"No weights for this layer\")\n        continue\n    for i, weight in enumerate(layer.weights):\n        print(\"Weights\", i+1)\n        print(\"Name\", weight.name)\n        print(\"Weights shape:\", weight.shape.as_list())"]},{"cell_type":"markdown","id":"8384943b-3fd8-4469-85ac-5b599954848e","metadata":{},"outputs":[],"source":["### Review of variational autoencoder loss:\n","\n","**Part 1:**\n","\n","We know Variational Autoencoders use the first part of their architecture - the encoder - to output zeros for the mean and zeros for the log variance (equal to a \"real\" variance of 1). \n","\n","We also know that the penalty for a given prediction of $\\mu_{pred}$ and $log(\\sigma)_{pred}$ is:\n","\n","$$\n","\\frac{1}{2} * (e^{log(\\sigma)_{pred}} - (1 + log(\\sigma)_{pred}) + (\\mu_{pred})^2 )\n","$$\n","\n","* **Note 1**: we predict $\\log(\\sigma)$ because predicting $\\sigma$ directly could result in a negative value, and having a negative value for the variance of a distribution makes no sense.\n","* **Note 2**: the cost function has two components, both of which penalize us for having results that deviate from a standard normal distribution.\n","    * The first part penalizes the $log(\\sigma)_{pred}$ from being away from 0, using the fact that $e^x - (x+1)$ is minimized at $x=0$.\n","    * The second part simply penalizes $\\mu_{pred}$ from being away from 0.\n","\n","This loss, representing a measure of the difference between these two distributions, is called the **KL Loss**.\n","\n","**Part 2:**\n","\n","The other part of the loss is simply the difference between the outputted image and the image fed in. The standard way of penalizing a difference between these two quantities is to use a `binary_crossentropy` function.\n","\n","### Exercise 4:\n","\n","Compute the loss for variational autoencoders. It should have two components:\n","\n","1. The reconstruction loss. Use the `binary_crossentropy` Keras function to compute the loss. \n","    \n","    **Note**: The `binary_crossentropy` function returns an average by default, so we  multiply this component of the loss by the number of pixels in the image (784) to get the total loss.\n","    \n","2. The KL loss. Use the formula above, and the `z_mean` and `z_log_var` functions to fill in the missing piece below.\n","    \n","    **Note**: We sum across the latent dimension to get a total loss for each input image.\n","    \n","Fill in the missing pieces:\n"]},{"cell_type":"code","id":"e1e238fb-a1c8-4556-b472-8e41baeff2c4","metadata":{},"outputs":[],"source":["reconstruction_loss = #your code here"]},{"cell_type":"markdown","id":"c65d00c4-eb53-4712-90d2-70ae61a7a937","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","reconstruction_loss = binary_crossentropy(inputs, outputs)\n","reconstruction_loss *= 784\n","\n","```\n","\n","</details>\n"]},{"cell_type":"code","id":"b87bc3f7-521a-4136-b840-9407fa0bcf64","metadata":{},"outputs":[],"source":["kl_loss = # YOUR CODE HERE"]},{"cell_type":"markdown","id":"543592d7-40db-4088-813a-e6555e854f87","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","kl_loss = 0.5 * (K.exp(z_log_var) - (1 + z_log_var) + K.square(z_mean))\n","kl_loss = K.sum(kl_loss, axis=-1)\n","\n","\n","```\n","\n","</details>\n"]},{"cell_type":"code","id":"94c835ce-bea4-4522-b620-3a8aa2e49ddc","metadata":{},"outputs":[],"source":["total_vae_loss = K.mean(reconstruction_loss + kl_loss)"]},{"cell_type":"markdown","id":"327937b3-c3b1-4748-836a-22ef62bf4df9","metadata":{},"outputs":[],"source":["### Compiling and fitting the model\n"]},{"cell_type":"code","id":"16f4e2e3-e644-4ea1-b137-102616b711f9","metadata":{},"outputs":[],"source":["vae_model.add_loss(total_vae_loss)\n\nvae_model.compile(optimizer='rmsprop',\n                  metrics=['accuracy'])\n    \nvae_model.summary()"]},{"cell_type":"code","id":"164d4f94-25f8-44b0-912e-641fd2378684","metadata":{},"outputs":[],"source":["vae_model.fit(x_train_flat,\n              x_train_flat,\n              epochs=epochs,\n              batch_size=batch_size)"]},{"cell_type":"code","id":"ceb72e27-cbc2-4671-a819-06c53014c57e","metadata":{},"outputs":[],"source":["history = full_model.fit(x_train_flat, x_train_flat, shuffle=True, epochs=1, batch_size=32)"]},{"cell_type":"markdown","id":"86c7d6cd-45d4-4137-9d34-786df2acbdce","metadata":{},"outputs":[],"source":["### Exercise 5:\n","\n","Compute the reconstruction error for the variational autoencoder. Is it higher or lower than for the autoencoder was after one epoch? Why do you think that is?\n"]},{"cell_type":"code","id":"23940e6a-bde1-41dd-831f-1ba6816f2ba1","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION\n# Generate reconstructed images\ndecoded_images = vae_model.predict(x_test_flat)\nmse_reconstruction(decoded_images, x_test_flat)\n### END SOLUTION"]},{"cell_type":"markdown","id":"e86f8c8a-9b6b-4cb0-94e0-9ae7056e17c6","metadata":{},"outputs":[],"source":["Much higher (\"worse\")! This is because a variational auto encoder is designed to maximize the interpretability of the latent space, _not_ to minimize reconstruction error. \n","\n","Below, we'll show how you can visualize the latent space that this variational autoencoder has learned:\n"]},{"cell_type":"markdown","id":"575fbed0-2549-4a67-a0fe-844bcc5e15cc","metadata":{},"outputs":[],"source":["#### Plotting the latent space:\n"]},{"cell_type":"code","id":"fbff9bee-3b11-4c85-8887-268f2d68c36c","metadata":{},"outputs":[],"source":["models = encoder_model, decoder_model \ndata = x_test_flat, y_test"]},{"cell_type":"code","id":"015d6347-1dce-45cc-9698-d3dba2c075a2","metadata":{},"outputs":[],"source":["def plot_results_var(models,\n                 data,\n                 batch_size=128,\n                 model_name=\"vae_mnist\",\n                    lim=4):\n    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n    # Arguments:\n        models (tuple): encoder and decoder models\n        data (tuple): test data and label\n        batch_size (int): prediction batch size\n        model_name (string): which model is using this function\n    \"\"\"\n\n    encoder, decoder = models\n    x_test, y_test = data\n    os.makedirs(model_name, exist_ok=True)\n\n    filename = os.path.join(model_name, \"vae_mean.png\")\n    \n    # display a 2D plot of the digit classes in the latent space\n    _, z_log_var, _ = encoder.predict(x_test,\n                                   batch_size=batch_size)\n    print(z_log_var)\n    plt.figure(figsize=(8, 7))\n    plt.scatter(z_log_var[:, 0], z_log_var[:, 1], c=y_test)\n    plt.colorbar()\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.savefig(filename)\n    plt.show()\n\n    filename = os.path.join(model_name, \"digits_over_latent.png\")\n    # display a 10x10 2D manifold of digits\n    n = 10\n    digit_size = 28\n    figure = np.zeros((digit_size * n, digit_size * n))\n    # linearly spaced coordinates corresponding to the 2D plot\n    # of digit classes in the latent space\n    grid_x = np.linspace(-1.0 * lim, lim, n)\n    grid_y = np.linspace(-1.0 * lim, lim, n)[::-1]\n\n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z_sample = np.array([[xi, yi]])\n            x_decoded = decoder.predict(z_sample, verbose=0)\n            digit = x_decoded[0].reshape(digit_size, digit_size)\n            figure[i * digit_size: (i + 1) * digit_size,\n                   j * digit_size: (j + 1) * digit_size] = digit\n\n    plt.figure(figsize=(8, 8))\n    start_range = digit_size // 2\n    end_range = n * digit_size + start_range + 1\n    pixel_range = np.arange(start_range, end_range, digit_size+1)\n    sample_range_x = np.round(grid_x, 1)\n    sample_range_y = np.round(grid_y, 1)\n    plt.xticks(pixel_range, sample_range_x)\n    plt.yticks(pixel_range, sample_range_y)\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.imshow(figure, cmap='Greys_r')\n    plt.savefig(filename)\n    plt.show()"]},{"cell_type":"code","id":"9eee4e72-ed67-44ab-8995-639c3c2492c3","metadata":{},"outputs":[],"source":["plot_results_var(models,\n             data,\n             batch_size=batch_size,\n             model_name=\"vae_mlp\", \n             lim=5)"]},{"cell_type":"markdown","id":"66a27c6e-9144-4f83-8c25-2099ece3e083","metadata":{},"outputs":[],"source":["Interesting results for just one epoch! As with the autoencoder above, you can modify the architecture of the VAE to try and produce better results, even adding convolutions.\n"]},{"cell_type":"markdown","id":"52f9c6b1-42a4-4a18-b59e-34e8a56bb771","metadata":{},"outputs":[],"source":["### Exercise 6\n","\n","Train VAE and AE for 10 epochs each, and plot the reconstruction MSE as a function of the number of epochs for each. Which one seems to have more potential to continuously learn as it is given more compute time? Is this surprising to you?\n"]},{"cell_type":"code","id":"2527a7b7-dbda-4449-a143-ee4354bb39f6","metadata":{},"outputs":[],"source":["### BEGIN SOLUTION \n# Gather AE loss \nloss_ae = train_ae_epochs(10)"]},{"cell_type":"code","id":"4d9aa3b8-f01c-4901-bcc8-f0c22686a94b","metadata":{},"outputs":[],"source":["# VAE model loss"]},{"cell_type":"code","id":"1c98010b-e173-4628-9095-f0ba59178f13","metadata":{},"outputs":[],"source":["vae_mse = []\nfor i in range(10):\n    vae_model.fit(x_train_flat,\n            epochs=1,\n            batch_size=batch_size)\n    decoded_images = vae_model.predict(x_test_flat)\n    vae_mse.append(mse_reconstruction(decoded_images, x_test_flat))"]},{"cell_type":"code","id":"80699762-4188-4b91-81a9-fc864f9ad795","metadata":{},"outputs":[],"source":["# Plot scaled losses\nplt.plot(range(10), loss_ae/(.01*loss_ae[0]), label='AE')\nplt.plot(range(10), vae_mse/(.01*vae_mse[0]), label='VAE')\nplt.xlabel('Epochs')\nplt.ylabel('MSE Recon. Loss (% of Epoch 1 loss)')\nplt.legend()\n### END SOLUTION "]},{"cell_type":"markdown","id":"501f39ba-9620-4f2c-bb09-847a13dff2dc","metadata":{},"outputs":[],"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"prev_pub_hash":"36a76ca090f637a84fb38d016e1236be0e2b7029193bfbea062ac47b1aefdb3c"},"nbformat":4,"nbformat_minor":4}