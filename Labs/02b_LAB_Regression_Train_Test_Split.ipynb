{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Machine Learning Foundation\n","\n","## Course 2, Part b: Regression Setup, Train-test Split LAB \n"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["## Introduction\n","\n","We will be working with a data set based on [housing prices in Ames, Iowa](https://www.kaggle.com/c/house-prices-advanced-regression-techniques?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork783-2023-01-01). It was compiled for educational use to be a modernized and expanded alternative to the well-known Boston Housing dataset. This version of the data set has had some missing values filled for convenience.\n","\n","There are an extensive number of features, so they've been described in the table below.\n","\n","### Predictor\n","\n","* SalePrice: The property's sale price in dollars. \n","\n","### Features\n","\n","* MoSold: Month Sold\n","* YrSold: Year Sold   \n","* SaleType: Type of sale\n","* SaleCondition: Condition of sale\n","* MSSubClass: The building class\n","* MSZoning: The general zoning classification\n","* ...\n"]},{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2017-03-09T17:24:40.724060Z","start_time":"2017-03-09T12:24:40.718739-05:00"},"run_control":{"marked":true}},"outputs":[],"source":["# Surpress warnings:\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["## Question 1\n","\n","* Import the data using Pandas and examine the shape. There are 79 feature columns plus the predictor, the sale price (`SalePrice`). \n","* There are three different types: integers (`int64`), floats (`float64`), and strings (`object`, categoricals). Examine how many there are of each data type. \n"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["(1379, 80)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","# Import the data using the file path\n","data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/data/Ames_Housing_Sales.csv\")\n","data.head()\n","\n","\n","print(data.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[{"data":{"text/plain":["object     43\n","float64    21\n","int64      16\n","Name: count, dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["data.dtypes.value_counts()"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["## Question 2\n","\n","A significant challenge, particularly when dealing with data that have many columns, is ensuring each column gets encoded correctly. \n","\n","This is particularly true with data columns that are ordered categoricals (ordinals) vs unordered categoricals. Unordered categoricals should be one-hot encoded, however this can significantly increase the number of features and creates features that are highly correlated with each other.\n","\n","Determine how many total features would be present, relative to what currently exists, if all string (object) features are one-hot encoded. Recall that the total number of one-hot encoded columns is `n-1`, where `n` is the number of categories.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[{"data":{"text/plain":["['Alley',\n"," 'BldgType',\n"," 'BsmtCond',\n"," 'BsmtExposure',\n"," 'BsmtFinType1',\n"," 'BsmtFinType2',\n"," 'BsmtQual',\n"," 'CentralAir',\n"," 'Condition1',\n"," 'Condition2',\n"," 'Electrical',\n"," 'ExterCond',\n"," 'ExterQual',\n"," 'Exterior1st',\n"," 'Exterior2nd',\n"," 'Fence',\n"," 'FireplaceQu',\n"," 'Foundation',\n"," 'Functional',\n"," 'GarageCond',\n"," 'GarageFinish',\n"," 'GarageQual',\n"," 'GarageType',\n"," 'Heating',\n"," 'HeatingQC',\n"," 'HouseStyle',\n"," 'KitchenQual',\n"," 'LandContour',\n"," 'LandSlope',\n"," 'LotConfig',\n"," 'LotShape',\n"," 'MSZoning',\n"," 'MasVnrType',\n"," 'MiscFeature',\n"," 'Neighborhood',\n"," 'PavedDrive',\n"," 'PoolQC',\n"," 'RoofMatl',\n"," 'RoofStyle',\n"," 'SaleCondition',\n"," 'SaleType',\n"," 'Street',\n"," 'Utilities']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Select the object (string) columns\n","categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n","categorical_cols"]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[{"data":{"text/plain":["204"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Determine how many extra columns would be created\n","num_ohc_cols = (data[categorical_cols]\n","                .apply(lambda x: x.nunique())\n","                .sort_values(ascending=False))\n","\n","\n","# No need to encode if there is only one value\n","small_num_ohc_cols = num_ohc_cols.loc[num_ohc_cols>1]\n","\n","# Number of one-hot columns is one less than the number of categories\n","small_num_ohc_cols -= 1\n","\n","# This is 215 columns, assuming the original ones are dropped.\n","# This is quite a few extra columns!\n","small_num_ohc_cols.sum()"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["## Question 3\n","\n","Let's create a new data set where all of the above categorical features will be one-hot encoded. We can fit this data and see how it affects the results.\n","\n","* Used the dataframe `.copy()` method to create a completely separate copy of the dataframe for one-hot encoding\n","* On this new dataframe, one-hot encode each of the appropriate columns and add it back to the dataframe. Be sure to drop the original column.\n","* For the data that are not one-hot encoded, drop the columns that are string categoricals.\n","\n","For the first step, numerically encoding the string categoricals, either Scikit-learn's `LabelEncoder` or `DictVectorizer` can be used. However, the former is probably easier since it doesn't require specifying a numerical value for each category, and we are going to one-hot encode all of the numerical values anyway. (Can you think of a time when `DictVectorizer` might be preferred?)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","\n","# Copy of the data\n","data_ohc = data.copy()\n","\n","# The encoders\n","le = LabelEncoder()\n","ohc = OneHotEncoder()\n","\n","for col in num_ohc_cols.index:\n","\n","    # Integer encode the string categories\n","    dat = le.fit_transform(data_ohc[col]).astype(np.int16)\n","\n","    # Remove the original column from the dataframe\n","    data_ohc = data_ohc.drop(col, axis=1)\n","\n","    # One hot encode the data--this returns a sparse array\n","    new_dat = ohc.fit_transform(dat.reshape(-1,1))\n","\n","    # Create unique column names\n","    n_cols = new_dat.shape[1]\n","    col_names = ['_'.join([col, str(x)]) for x in range(n_cols)]\n","\n","    # Create the new dataframe\n","    new_df = pd.DataFrame(new_dat.toarray(),\n","                          index=data_ohc.index,\n","                          columns=col_names)\n","\n","    # Append the new data to the dataframe\n","    data_ohc = pd.concat([data_ohc, new_df], axis=1)"]},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[{"data":{"text/plain":["215"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Column difference is as calculated above\n","data_ohc.shape[1] - data.shape[1]"]},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["80\n","37\n"]}],"source":["print(data.shape[1])\n","\n","# Remove the string columns from the dataframe\n","data = data.drop(num_ohc_cols.index, axis=1)\n","\n","print(data.shape[1])"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["## Question 4\n","\n","* Create train and test splits of both data sets. To ensure the data gets split the same way, use the same `random_state` in each of the two splits.\n","* For each data set, fit a basic linear regression model on the training data. \n","* Calculate the mean squared error on both the train and test sets for the respective models. Which model produces smaller error on the test data and why?\n"]},{"cell_type":"code","execution_count":14,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","y_col = 'SalePrice'\n","\n","# Split the data that is not one-hot encoded\n","feature_cols = [x for x in data.columns if x != y_col]\n","X_data = data[feature_cols]\n","y_data = data[y_col]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,\n","                                                    test_size=0.3, random_state=42)\n","# Split the data that is one-hot encoded\n","feature_cols = [x for x in data_ohc.columns if x != y_col]\n","X_data_ohc = data_ohc[feature_cols]\n","y_data_ohc = data_ohc[y_col]\n","\n","X_train_ohc, X_test_ohc, y_train_ohc, y_test_ohc = train_test_split(X_data_ohc, y_data_ohc,\n","                                                    test_size=0.3, random_state=42)"]},{"cell_type":"code","execution_count":15,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[{"data":{"text/plain":["True"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Compare the indices to ensure they are identical\n","(X_train_ohc.index == X_train.index).all()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1stFlrSF</th>\n","      <th>2ndFlrSF</th>\n","      <th>3SsnPorch</th>\n","      <th>BedroomAbvGr</th>\n","      <th>BsmtFinSF1</th>\n","      <th>BsmtFinSF2</th>\n","      <th>BsmtFullBath</th>\n","      <th>BsmtHalfBath</th>\n","      <th>BsmtUnfSF</th>\n","      <th>EnclosedPorch</th>\n","      <th>...</th>\n","      <th>OverallCond</th>\n","      <th>OverallQual</th>\n","      <th>PoolArea</th>\n","      <th>ScreenPorch</th>\n","      <th>TotRmsAbvGrd</th>\n","      <th>TotalBsmtSF</th>\n","      <th>WoodDeckSF</th>\n","      <th>YearBuilt</th>\n","      <th>YearRemodAdd</th>\n","      <th>YrSold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>461</th>\n","      <td>630.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>515.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>115.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>8</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>630.0</td>\n","      <td>0.0</td>\n","      <td>1970</td>\n","      <td>2002</td>\n","      <td>2009</td>\n","    </tr>\n","    <tr>\n","      <th>976</th>\n","      <td>845.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5</td>\n","      <td>0.0</td>\n","      <td>186.0</td>\n","      <td>1957</td>\n","      <td>1957</td>\n","      <td>2009</td>\n","    </tr>\n","    <tr>\n","      <th>1128</th>\n","      <td>728.0</td>\n","      <td>728.0</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>728.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8</td>\n","      <td>728.0</td>\n","      <td>100.0</td>\n","      <td>2005</td>\n","      <td>2005</td>\n","      <td>2008</td>\n","    </tr>\n","    <tr>\n","      <th>904</th>\n","      <td>561.0</td>\n","      <td>668.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>285.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>276.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5</td>\n","      <td>561.0</td>\n","      <td>150.0</td>\n","      <td>1980</td>\n","      <td>1980</td>\n","      <td>2009</td>\n","    </tr>\n","    <tr>\n","      <th>506</th>\n","      <td>1601.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>1358.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>223.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>8</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6</td>\n","      <td>1581.0</td>\n","      <td>180.0</td>\n","      <td>2001</td>\n","      <td>2002</td>\n","      <td>2010</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1095</th>\n","      <td>855.0</td>\n","      <td>601.0</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>311.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>544.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7</td>\n","      <td>855.0</td>\n","      <td>26.0</td>\n","      <td>1978</td>\n","      <td>1978</td>\n","      <td>2010</td>\n","    </tr>\n","    <tr>\n","      <th>1130</th>\n","      <td>815.0</td>\n","      <td>875.0</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>815.0</td>\n","      <td>330.0</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>7</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7</td>\n","      <td>815.0</td>\n","      <td>0.0</td>\n","      <td>1916</td>\n","      <td>1950</td>\n","      <td>2006</td>\n","    </tr>\n","    <tr>\n","      <th>1294</th>\n","      <td>1661.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>831.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>161.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>0.0</td>\n","      <td>178.0</td>\n","      <td>8</td>\n","      <td>992.0</td>\n","      <td>0.0</td>\n","      <td>1955</td>\n","      <td>1996</td>\n","      <td>2008</td>\n","    </tr>\n","    <tr>\n","      <th>860</th>\n","      <td>742.0</td>\n","      <td>742.0</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>742.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8</td>\n","      <td>742.0</td>\n","      <td>36.0</td>\n","      <td>2005</td>\n","      <td>2005</td>\n","      <td>2009</td>\n","    </tr>\n","    <tr>\n","      <th>1126</th>\n","      <td>1224.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","      <td>883.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>341.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5</td>\n","      <td>1224.0</td>\n","      <td>0.0</td>\n","      <td>1999</td>\n","      <td>1999</td>\n","      <td>2009</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>965 rows Ã— 36 columns</p>\n","</div>"],"text/plain":["      1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n","461      630.0       0.0        0.0             1       515.0         0.0   \n","976      845.0       0.0        0.0             3         0.0         0.0   \n","1128     728.0     728.0        0.0             3         0.0         0.0   \n","904      561.0     668.0        0.0             2       285.0         0.0   \n","506     1601.0       0.0        0.0             3      1358.0         0.0   \n","...        ...       ...        ...           ...         ...         ...   \n","1095     855.0     601.0        0.0             3       311.0         0.0   \n","1130     815.0     875.0        0.0             3         0.0         0.0   \n","1294    1661.0       0.0        0.0             3       831.0         0.0   \n","860      742.0     742.0        0.0             3         0.0         0.0   \n","1126    1224.0       0.0        0.0             2       883.0         0.0   \n","\n","      BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch  ...  OverallCond  \\\n","461              1             0      115.0            0.0  ...            8   \n","976              0             0        0.0            0.0  ...            3   \n","1128             0             0      728.0            0.0  ...            5   \n","904              0             0      276.0            0.0  ...            6   \n","506              1             0      223.0            0.0  ...            5   \n","...            ...           ...        ...            ...  ...          ...   \n","1095             0             0      544.0            0.0  ...            5   \n","1130             0             0      815.0          330.0  ...            6   \n","1294             1             0      161.0            0.0  ...            6   \n","860              0             0      742.0            0.0  ...            5   \n","1126             1             0      341.0            0.0  ...            5   \n","\n","      OverallQual  PoolArea  ScreenPorch  TotRmsAbvGrd  TotalBsmtSF  \\\n","461             4       0.0          0.0             3        630.0   \n","976             4       0.0          0.0             5          0.0   \n","1128            6       0.0          0.0             8        728.0   \n","904             6       0.0          0.0             5        561.0   \n","506             8       0.0          0.0             6       1581.0   \n","...           ...       ...          ...           ...          ...   \n","1095            6       0.0          0.0             7        855.0   \n","1130            7       0.0          0.0             7        815.0   \n","1294            6       0.0        178.0             8        992.0   \n","860             6       0.0          0.0             8        742.0   \n","1126            6       0.0          0.0             5       1224.0   \n","\n","      WoodDeckSF  YearBuilt  YearRemodAdd  YrSold  \n","461          0.0       1970          2002    2009  \n","976        186.0       1957          1957    2009  \n","1128       100.0       2005          2005    2008  \n","904        150.0       1980          1980    2009  \n","506        180.0       2001          2002    2010  \n","...          ...        ...           ...     ...  \n","1095        26.0       1978          1978    2010  \n","1130         0.0       1916          1950    2006  \n","1294         0.0       1955          1996    2008  \n","860         36.0       2005          2005    2009  \n","1126         0.0       1999          1999    2009  \n","\n","[965 rows x 36 columns]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["X_train"]},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>no enc</th>\n","      <th>one-hot enc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>train</th>\n","      <td>1.131507e+09</td>\n","      <td>3.177275e+08</td>\n","    </tr>\n","    <tr>\n","      <th>test</th>\n","      <td>1.372182e+09</td>\n","      <td>7.955146e+16</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             no enc   one-hot enc\n","train  1.131507e+09  3.177275e+08\n","test   1.372182e+09  7.955146e+16"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","LR = LinearRegression()\n","\n","# Storage for error values\n","error_df = list()\n","\n","# Data that have not been one-hot encoded\n","LR = LR.fit(X_train, y_train)\n","y_train_pred = LR.predict(X_train)\n","y_test_pred = LR.predict(X_test)\n","\n","error_df.append(pd.Series({'train': mean_squared_error(y_train, y_train_pred),\n","                           'test' : mean_squared_error(y_test,  y_test_pred)},\n","                           name='no enc'))\n","\n","# Data that have been one-hot encoded\n","LR = LR.fit(X_train_ohc, y_train_ohc)\n","y_train_ohc_pred = LR.predict(X_train_ohc)\n","y_test_ohc_pred = LR.predict(X_test_ohc)\n","\n","error_df.append(pd.Series({'train': mean_squared_error(y_train_ohc, y_train_ohc_pred),\n","                           'test' : mean_squared_error(y_test_ohc,  y_test_ohc_pred)},\n","                          name='one-hot enc'))\n","\n","# Assemble the results\n","error_df = pd.concat(error_df, axis=1)\n","error_df"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["Note that the error values on the one-hot encoded data are very different for the train and test data. In particular, the errors on the test data are much higher. Based on the lecture, this is because the one-hot encoded model is overfitting the data. We will learn how to deal with issues like this in the next lesson.\n"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["## Question 5\n","\n","For each of the data sets (one-hot encoded and not encoded):\n","\n","* Scale the all the non-hot encoded values using one of the following: `StandardScaler`, `MinMaxScaler`, `MaxAbsScaler`.\n","* Compare the error calculated on the test sets\n","\n","Be sure to calculate the skew (to decide if a transformation should be done) and fit the scaler on *ONLY* the training data, but then apply it to both the train and test data identically.\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# Mute the setting wtih a copy warnings\n","pd.options.mode.chained_assignment = None"]},{"cell_type":"code","execution_count":25,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[{"ename":"ValueError","evalue":"at least one array or dtype is required","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m/Users/vanstrouble/Repositorios/ml-ibm-certificate/Labs/02b_LAB_Regression_Train_Test_Split.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vanstrouble/Repositorios/ml-ibm-certificate/Labs/02b_LAB_Regression_Train_Test_Split.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m trainingset \u001b[39m=\u001b[39m _X_train\u001b[39m.\u001b[39mcopy()  \u001b[39m# copy because we dont want to scale this more than once.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vanstrouble/Repositorios/ml-ibm-certificate/Labs/02b_LAB_Regression_Train_Test_Split.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m testset \u001b[39m=\u001b[39m _X_test\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vanstrouble/Repositorios/ml-ibm-certificate/Labs/02b_LAB_Regression_Train_Test_Split.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m trainingset[float_columns] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(trainingset[float_columns])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vanstrouble/Repositorios/ml-ibm-certificate/Labs/02b_LAB_Regression_Train_Test_Split.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m testset[float_columns] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(testset[float_columns])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vanstrouble/Repositorios/ml-ibm-certificate/Labs/02b_LAB_Regression_Train_Test_Split.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m LR\u001b[39m.\u001b[39mfit(trainingset, _y_train)\n","File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ibm-cert/lib/python3.11/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n","File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ibm-cert/lib/python3.11/site-packages/sklearn/base.py:916\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    917\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n","File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ibm-cert/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:839\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 839\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n","File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ibm-cert/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ibm-cert/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:875\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \n\u001b[1;32m    845\u001b[0m \u001b[39mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[39m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    874\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 875\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    876\u001b[0m     X,\n\u001b[1;32m    877\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    878\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m    879\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    880\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[1;32m    881\u001b[0m )\n\u001b[1;32m    882\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    884\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ibm-cert/lib/python3.11/site-packages/sklearn/base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    603\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    604\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 605\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    606\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    607\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n","File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ibm-cert/lib/python3.11/site-packages/sklearn/utils/validation.py:795\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    791\u001b[0m pandas_requires_conversion \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\n\u001b[1;32m    792\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dtypes_orig\n\u001b[1;32m    793\u001b[0m )\n\u001b[1;32m    794\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(dtype_iter, np\u001b[39m.\u001b[39mdtype) \u001b[39mfor\u001b[39;00m dtype_iter \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[0;32m--> 795\u001b[0m     dtype_orig \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mresult_type(\u001b[39m*\u001b[39;49mdtypes_orig)\n\u001b[1;32m    796\u001b[0m \u001b[39melif\u001b[39;00m pandas_requires_conversion \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(d \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dtypes_orig):\n\u001b[1;32m    797\u001b[0m     \u001b[39m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[1;32m    798\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m\n","\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"]}],"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n","\n","\n","scalers = {'standard': StandardScaler(),\n","           'minmax': MinMaxScaler(),\n","           'maxabs': MaxAbsScaler()}\n","\n","training_test_sets = {\n","    'not_encoded': (X_train, y_train, X_test, y_test),\n","    'one_hot_encoded': (X_train_ohc, y_train_ohc, X_test_ohc, y_test_ohc)}\n","\n","\n","# Get the list of float columns, and the float data\n","# so that we don't scale something we already scaled.\n","# We're supposed to scale the original data each time\n","mask = X_train.dtypes == np.float32\n","float_columns = X_train.columns[mask]\n","\n","# initialize model\n","LR = LinearRegression()\n","\n","# iterate over all possible combinations and get the errors\n","errors = {}\n","for encoding_label, (_X_train, _y_train, _X_test, _y_test) in training_test_sets.items():\n","    for scaler_label, scaler in scalers.items():\n","        trainingset = _X_train.copy()  # copy because we dont want to scale this more than once.\n","        testset = _X_test.copy()\n","        trainingset[float_columns] = scaler.fit_transform(trainingset[float_columns])\n","        testset[float_columns] = scaler.transform(testset[float_columns])\n","        LR.fit(trainingset, _y_train)\n","        predictions = LR.predict(testset)\n","        key = encoding_label + ' - ' + scaler_label + 'scaling'\n","        errors[key] = mean_squared_error(_y_test, predictions)\n","\n","errors = pd.Series(errors)\n","print(errors.to_string())\n","print('-' * 80)\n","for key, error_val in errors.items():\n","    print(key, error_val)"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["## Question 6\n","\n","Plot predictions vs actual for one of the models.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"run_control":{"marked":true}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","\n","sns.set_context('talk')\n","sns.set_style('ticks')\n","sns.set_palette('dark')\n","\n","ax = plt.axes()\n","# we are going to use y_test, y_test_pred\n","ax.scatter(y_test, y_test_pred, alpha=.5)\n","\n","ax.set(xlabel='Ground truth',\n","       ylabel='Predictions',\n","       title='Ames, Iowa House Price Predictions vs Truth, using Linear Regression');"]},{"cell_type":"markdown","metadata":{},"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"ibm-cert","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
